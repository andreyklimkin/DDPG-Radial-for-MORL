{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import ou_noise\n",
    "import f2_noise\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pendulumMulti_environment import PendulumEnvMulti\n",
    "#Environment state size\n",
    "State_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create folder for debug info and plots\n",
    "save_plots_dir = \"results_pend_horizontal\"\n",
    "!mkdir results_pend_horizontal\n",
    "!mkdir results_pend_horizontal/reward_plots\n",
    "!mkdir results_pend_horizontal/action_plots\n",
    "!mkdir results_pend_horizontal/loss_plots\n",
    "!mkdir results_pend_horizontal/gradient_norm_plots\n",
    "!mkdir results_pend_horizontal/term_stats\n",
    "!mkdir results_pend_horizontal/pareto_front_progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choose gradient weights for reward components\n",
    "gradient_weights = list()\n",
    "for w in np.linspace(0, 1, 21):\n",
    "    gradient_weights.append(np.array([w, float(\"{0:.2f}\".format(1-w))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.,  1.]),\n",
       " array([ 0.05,  0.95]),\n",
       " array([ 0.1,  0.9]),\n",
       " array([ 0.15,  0.85]),\n",
       " array([ 0.2,  0.8]),\n",
       " array([ 0.25,  0.75]),\n",
       " array([ 0.3,  0.7]),\n",
       " array([ 0.35,  0.65]),\n",
       " array([ 0.4,  0.6]),\n",
       " array([ 0.45,  0.55]),\n",
       " array([ 0.5,  0.5]),\n",
       " array([ 0.55,  0.45]),\n",
       " array([ 0.6,  0.4]),\n",
       " array([ 0.65,  0.35]),\n",
       " array([ 0.7,  0.3]),\n",
       " array([ 0.75,  0.25]),\n",
       " array([ 0.8,  0.2]),\n",
       " array([ 0.85,  0.15]),\n",
       " array([ 0.9,  0.1]),\n",
       " array([ 0.95,  0.05]),\n",
       " array([ 1.,  0.])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Render gym env during training\n",
    "RENDER_ENV = False\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = False\n",
    "# Gym environment\n",
    "\n",
    "ENV_NAME = 'PendulumMulti-v0'\n",
    "\n",
    "# Directory for storing gym results\n",
    "MONITOR_DIR = './results_pend_horizontal/videos_pend'\n",
    "# Directory for storing tensorboard summary results\n",
    "SUMMARY_DIR = './results_pend_horizontal/tf_ddpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from actor_critic_networks import *\n",
    "from TerminateChecker import TerminateChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dominates(a, b):\n",
    "    for ai, bi in zip(a, b):\n",
    "        if(bi > ai):\n",
    "            return False\n",
    "    if(np.all(a == b)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def remove_dominated(xs):\n",
    "    is_dominated = np.zeros(xs.shape[0])\n",
    "    \n",
    "    for i in range(xs.shape[0]):\n",
    "        for j in range(xs.shape[0]):\n",
    "            if(i != j and dominates(xs[i], xs[j])):\n",
    "                is_dominated[j] = 1\n",
    "                \n",
    "    return xs[is_dominated == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "def train(sess, env, actor, critic, action_dim, action_bound, grad_weights):\n",
    "    \n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    small_replay_buffer = ReplayBuffer(SMALL_BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    eps_reward = list()\n",
    "    #constant for scaling the immediate reward\n",
    "    REWARD_BURN_CONST = 1\n",
    "    \n",
    "    reward_timestep_sum = 0\n",
    "    noise_level = 1.0\n",
    "    \n",
    "    #storing loss and gradient norm\n",
    "    draw_loss = list()\n",
    "    draw_global_norm = list()\n",
    "    draw_layers_norm = list()   \n",
    "    draw_critic_norm = list()\n",
    "    draw_actor_norm = list()\n",
    "    \n",
    "    terminate_statistics = list()\n",
    "    TERMINATE_TEST_SIZE = 50\n",
    "    CHECK_FOR_PROFIT_TIMES = 50\n",
    "    \n",
    "    for i in range(MAX_EPISODES):\n",
    "        \n",
    "        if(i == 1):\n",
    "            checker = TerminateChecker(eps_reward[0], 0.05, TERMINATE_TEST_SIZE, CHECK_FOR_PROFIT_TIMES)\n",
    "        \n",
    "        if(i >= TERMINATE_TEST_SIZE):\n",
    "            \n",
    "            if(checker.terminate_check()):\n",
    "                print(\"#\" * 50)\n",
    "                print(\"STOP TRAINING in {} episodes\".format(str(i + 1)))\n",
    "                print(\"Weights are:\", grad_weights)\n",
    "                break\n",
    "            \n",
    "            terminate_statistics.append(np.array([checker.ma, checker.ema]))\n",
    "            fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 15))\n",
    "            ax[0].set_title(\"#Episode/Reward\")\n",
    "            ax[0].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[0].set_ylabel(\"Reward\", fontsize=14)    \n",
    "            ax[0].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), eps_reward[-len(terminate_statistics):])\n",
    "            \n",
    "            ax[1].set_title(\"#Episode/MA value\")\n",
    "            ax[1].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[1].set_ylabel(\"MA Value\", fontsize=14)    \n",
    "            ax[1].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), np.array(terminate_statistics)[:, 0])\n",
    "            start_x = TERMINATE_TEST_SIZE\n",
    "            finish_x = len(terminate_statistics) + TERMINATE_TEST_SIZE\n",
    "            ax[1].plot([start_x, finish_x], [checker.best_ma, checker.best_ma], color=\"red\")\n",
    "            ax[1].vlines(np.argmax(np.array(terminate_statistics)[:, 0])+(TERMINATE_TEST_SIZE), np.min(eps_reward), np.max(eps_reward), color=\"green\")\n",
    "            ax[1].set_ylim([np.min(eps_reward)-100, np.max(eps_reward)+100])\n",
    "            \n",
    "            ax[2].set_title(\"#Episode/EMA value\")\n",
    "            ax[2].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[2].set_ylabel(\"EMA Value\", fontsize=14)    \n",
    "            ax[2].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), np.array(terminate_statistics)[:, 1])\n",
    "            ax[2].plot([start_x, finish_x], [checker.best_ema, checker.best_ema], color=\"red\")\n",
    "            ax[2].set_ylim([np.min(eps_reward)-100, np.max(eps_reward)+100])\n",
    "            ax[2].vlines(np.argmax(np.array(terminate_statistics)[:, 1])+(TERMINATE_TEST_SIZE), np.min(eps_reward), np.max(eps_reward), color=\"green\")\n",
    "            \n",
    "            plt.savefig(save_plots_dir + \"/term_stats/terminate_statistics_plot_\" + str(grad_weights) + \".png\")\n",
    "        \n",
    "        noise_level *= 0.95\n",
    "        noise_level = max(1e-7,noise_level - 1e-4)\n",
    "        \n",
    "        #for plotting\n",
    "        grad_global_norm = list()\n",
    "        grad_layers_norm = list()\n",
    "        episode_actor_grads = list()\n",
    "        REWARD_PLOT_TIMESTEP = 1\n",
    "        ACTION_PLOT_TIMESTEP = 10\n",
    "        #constant for plotting smoothed reward(averaged by last SMOOTH_REWARD episodes)\n",
    "        SMOOTH_REWARD = 10\n",
    "        predicted_actions = list()\n",
    "        noise_actions = list()\n",
    "\n",
    "        ep_reward = np.zeros(REWARD_SPACE_DIM)\n",
    "        ep_ave_max_q = np.zeros(REWARD_SPACE_DIM)\n",
    "        \n",
    "        state_reward = list()\n",
    "        loss = list()\n",
    "        \n",
    "        s = env.reset()\n",
    "        for j in range(MAX_EP_STEPS):\n",
    "\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "\n",
    "            # Added exploration noise\n",
    "            predicted_action = actor.predict(np.reshape(s, (1, State_size)))\n",
    "            noise_action = actor.noise.one(action_dim, noise_level)\n",
    "            a = np.clip(predicted_action + noise_action, -action_bound, action_bound)\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "            r *= REWARD_BURN_CONST\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "            \n",
    "            small_replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "            \n",
    "            if(i % ACTION_PLOT_TIMESTEP == 0):\n",
    "                predicted_actions.append(predicted_action[0])\n",
    "                noise_actions.append(noise_action)\n",
    "                state_reward.append(r)\n",
    "            \n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            calculated_grad = False\n",
    "            \n",
    "            if replay_buffer.size() >= MINIBATCH_SIZE:\n",
    "                \n",
    "                for sample_num in range(R):\n",
    "                    \n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(batch_size=MINIBATCH_SIZE)\n",
    "\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                        s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                    y_i = []\n",
    "                    for k in range(MINIBATCH_SIZE):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                    predicted_q_value, _ = critic.train(\n",
    "                        s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, REWARD_SPACE_DIM)))\n",
    "                    \n",
    "                    gain_predictions = np.mean(critic.predict(s_batch, actor.predict(s_batch)), axis=0)\n",
    "                    \n",
    "                    loss.append(np.mean((gain_predictions).dot(grad_weights)))\n",
    "\n",
    "                # Update the actor policy using the sampled gradient\n",
    "                    a_outs = actor.predict(s_batch)\n",
    "                    grads = critic.action_gradients(s_batch, a_outs, np.reshape(grad_weights, (REWARD_SPACE_DIM, 1)))\n",
    "                    _, global_norm, layers_norm = actor.train(s_batch, grads[0])\n",
    "                    \n",
    "                    grad_global_norm.append(global_norm)\n",
    "                    grad_layers_norm.append(layers_norm)\n",
    "                    \n",
    "                    #episode_actor_grads.append(actor_grad)\n",
    "                    actor.update_target_network()\n",
    "                    critic.update_target_network()\n",
    "            \n",
    "            s = s2\n",
    "            ep_reward += (np.array(r) * (GAMMA ** j)) / REWARD_BURN_CONST\n",
    "            \n",
    "            if(j == MAX_EP_STEPS - 1):\n",
    "                terminal = True\n",
    "                \n",
    "            if(terminal):\n",
    "                cost = np.sum(ep_reward * grad_weights)\n",
    "                reward_timestep_sum += cost\n",
    "                eps_reward.append(cost)        \n",
    "                \n",
    "                ###DRAW PLOTS BEGINING\n",
    "                if(i % REWARD_PLOT_TIMESTEP == 0):\n",
    "                    #eps_reward.append(reward_timestep_sum / REWARD_PLOT_TIMESTEP)\n",
    "                    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "                    ax.set_title(\"Episode/Cumulative Reward\", fontsize=18)\n",
    "                    ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                    ax.set_ylabel(\"Cumulative Reward\", fontsize=14)\n",
    "                    ax.plot((np.arange(len(eps_reward)) + 1) * REWARD_PLOT_TIMESTEP, eps_reward)\n",
    "                    reward_timestep_sum = 0\n",
    "                    plt.savefig(save_plots_dir + \"/reward_plots/episodes_reward_plot\")\n",
    "\n",
    "                if((i + 1) % SMOOTH_REWARD == 0 and (i + 1) >= SMOOTH_REWARD):\n",
    "                    smoothed_reward = np.mean(np.array(eps_reward).reshape((len(eps_reward) // SMOOTH_REWARD, SMOOTH_REWARD)), axis=1)\n",
    "                    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "                    ax.set_title(\"Episode/Smoothed Cumulative Reward\", fontsize=18)\n",
    "                    ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                    ax.set_ylabel(\"Smoothed Cumulative Reward\", fontsize=14)\n",
    "                    ax.plot(np.arange(smoothed_reward.shape[0]) * SMOOTH_REWARD + 1, smoothed_reward)\n",
    "                    plt.savefig(save_plots_dir + \"/reward_plots/smoothed_episodes_reward_plot\")\n",
    "\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "\n",
    "                draw_global_norm.append(np.mean(np.array(grad_global_norm)))\n",
    "                ax.plot(draw_global_norm)\n",
    "                ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax.set_ylabel(\"Average gradient norm for all layers\", fontsize=14)\n",
    "                ax.set_title(\"#Episode/Gradient Norm\", fontsize=18)\n",
    "\n",
    "                plt.savefig(save_plots_dir + \"/gradient_norm_plots/episodes_gradient_norm_plot\")\n",
    "\n",
    "                draw_loss.append(-np.mean(np.array(loss)))\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "                ax.set_title(\"#Episode/Loss\")\n",
    "                ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax.set_ylabel(\"Loss\", fontsize=14)\n",
    "                ax.plot(draw_loss)\n",
    "                plt.savefig(save_plots_dir + \"/loss_plots/episodes_loss_plot\")\n",
    "        \n",
    "                ### DRAW PLOTS ENDING\n",
    "                plt.close(\"all\")\n",
    "                \n",
    "                #Update Statistics\n",
    "                if(i >= 1):\n",
    "                    checker.change_ma(eps_reward[-1])\n",
    "                    checker.change_ema(eps_reward[-1])\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "def test(weights, actor, test_episodes_num, action_bound):\n",
    "    \n",
    "    video_cal = lambda x : x % 25 == 0\n",
    "\n",
    "    env = PendulumEnvMulti()\n",
    "    \n",
    "    eps_reward = list()\n",
    "    \n",
    "    print(\"!!!!!TEST WITH WEIGHTS : \", weights)\n",
    "    \n",
    "    for i in range(test_episodes_num):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        cum_reward = np.zeros(REWARD_SPACE_DIM)\n",
    "        \n",
    "        for j in range(MAX_EP_STEPS):\n",
    "\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "            a = actor.predict(np.reshape(s, (1, State_size)))\n",
    "            a = np.clip(a, -action_bound, action_bound)\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "            if(j == MAX_EP_STEPS - 1):\n",
    "                terminal = True\n",
    "                \n",
    "            cum_reward += (r * (GAMMA ** j))\n",
    "            \n",
    "            s = s2\n",
    "            \n",
    "            if(j == MAX_EP_STEPS - 1):\n",
    "                terminal = True\n",
    "            \n",
    "            if(terminal):\n",
    "                eps_reward.append(cum_reward)\n",
    "                break\n",
    "    \n",
    "    eps_reward = np.mean(eps_reward, axis=0)\n",
    "    \n",
    "    return eps_reward\n",
    "  \n",
    "def main():\n",
    "    \n",
    "    timer = time.time()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        env = PendulumEnvMulti()    \n",
    "        np.random.seed(RANDOM_SEED)\n",
    "        tf.set_random_seed(RANDOM_SEED)\n",
    "        env.seed(RANDOM_SEED)\n",
    "\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_bound = env.action_space.high\n",
    "        \n",
    "        for ind, w in tqdm(enumerate(gradient_weights)):\n",
    "            env = PendulumEnvMulti()\n",
    "            actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                             ACTOR_LEARNING_RATE, TAU)\n",
    "\n",
    "            critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())\n",
    "        \n",
    "            actor.noise = f2_noise.one_fsq_noise()\n",
    "        \n",
    "            video_cal = lambda x : x % 20 == 0\n",
    "        \n",
    "            if GYM_MONITOR_EN:\n",
    "                if not RENDER_ENV:\n",
    "                    env = wrappers.Monitor(\n",
    "                        env, MONITOR_DIR, video_callable=False, force=True)\n",
    "                else:\n",
    "                    env = wrappers.Monitor(env, MONITOR_DIR, video_callable=video_cal, force=True)\n",
    "            train(sess, env, actor, critic, action_dim, action_bound, w)\n",
    "\n",
    "            env.close()\n",
    "            \n",
    "            test(w, actor, MAX_EPISODES_TEST, action_bound)\n",
    "        \n",
    "            pareto[str(w)] = test(w, actor, MAX_EPISODES_TEST, action_bound)\n",
    "            \n",
    "            front = remove_dominated(np.array([y for y in pareto.values()]))\n",
    "            np.save(save_plots_dir + \"/pareto_front_progress/\" + str(ind + 1) + str(\"_weights_samples\"), front)\n",
    "            file_pareto = open(save_plots_dir + \"/pareto_dict\", \"w\")\n",
    "            file_pareto.write(str(pareto))\n",
    "            file_pareto.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Max Number of Training Episodes\n",
    "MAX_EPISODES = 1000\n",
    "#Number ot test episodes\n",
    "MAX_EPISODES_TEST = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing all found policies\n",
    "pareto = dict()\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
