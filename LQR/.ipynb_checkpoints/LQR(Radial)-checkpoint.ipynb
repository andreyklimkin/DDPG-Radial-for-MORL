{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import ou_noise\n",
    "import f2_noise\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from LQR_environment import LQR\n",
    "#Environment state size\n",
    "State_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create folder for debug info and plots\n",
    "save_plots_dir = \"results_lqr\"\n",
    "!mkdir results_lqr\n",
    "!mkdir results_lqr/reward_plots\n",
    "!mkdir results_lqr/action_plots\n",
    "!mkdir results_lqr/loss_plots\n",
    "!mkdir results_lqr/gradient_norm_plots\n",
    "!mkdir results_lqr/pareto_front_progress\n",
    "!mkdir results_lqr/term_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#choose gradient weights for reward components\n",
    "gradient_weights = []\n",
    "for w in np.linspace(0, 1, 11):\n",
    "    gradient_weights.append([w, float(\"{0:.2f}\".format(1-w))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 1.0],\n",
       " [0.10000000000000001, 0.9],\n",
       " [0.20000000000000001, 0.8],\n",
       " [0.30000000000000004, 0.7],\n",
       " [0.40000000000000002, 0.6],\n",
       " [0.5, 0.5],\n",
       " [0.60000000000000009, 0.4],\n",
       " [0.70000000000000007, 0.3],\n",
       " [0.80000000000000004, 0.2],\n",
       " [0.90000000000000002, 0.1],\n",
       " [1.0, 0.0]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Render gym env during training\n",
    "RENDER_ENV = False\n",
    "# Use Gym Monitor\n",
    "GYM_MONITOR_EN = False\n",
    "\n",
    "# Gym environment\n",
    "#ENV_NAME = 'Pendulum-v0'\n",
    "#ENV_NAME = 'PendulumMulti-v0'\n",
    "#ENV_NAME = 'BipedalWalker-v2'\n",
    "ENV_NAME = 'LQR'\n",
    "\n",
    "# Directory for storing gym results\n",
    "MONITOR_DIR = './'+ save_plots_dir +'/videos_pend'\n",
    "# Directory for storing tensorboard summary results\n",
    "SUMMARY_DIR = './'+ save_plots_dir +'/tf_ddpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from actor_critic_networks import *\n",
    "from TerminateChecker import TerminateChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#methods for removing pareto dominated solutions\n",
    "def dominates(a, b):\n",
    "    for ai, bi in zip(a, b):\n",
    "        if(bi > ai):\n",
    "            return False\n",
    "    if(np.all(a == b)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def remove_dominated(xs):\n",
    "    is_dominated = np.zeros(xs.shape[0])\n",
    "    \n",
    "    for i in range(xs.shape[0]):\n",
    "        for j in range(xs.shape[0]):\n",
    "            if(i != j and dominates(xs[i], xs[j])):\n",
    "                is_dominated[j] = 1\n",
    "                \n",
    "    return xs[is_dominated == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars\n",
    "\n",
    "def train(sess, env, actor, critic, file_name, action_dim, action_bound, grad_weights, log_file):\n",
    "    best_cost = -float(\"inf\")\n",
    "    best_rewards = np.zeros(2)\n",
    "    \n",
    "    log_file.write(\"CURRENT WEIGHTS ARE: \" + str(grad_weights))\n",
    "    \n",
    "    # Set up summary Ops\n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "\n",
    "    # Initialize target network weights\n",
    "    actor.update_target_network()\n",
    "    critic.update_target_network()\n",
    "\n",
    "    # Initialize replay memory\n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    small_replay_buffer = ReplayBuffer(SMALL_BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    eps_reward = list()\n",
    "    #constant for scaling the immediate reward\n",
    "    REWARD_BURN_CONST = 1e-5\n",
    "    \n",
    "    reward_timestep_sum = 0\n",
    "    noise_level = 2.0\n",
    "    \n",
    "    #for storing loss and gradient norm\n",
    "    draw_loss = list()\n",
    "    draw_global_norm = list()\n",
    "    draw_layers_norm = list()   \n",
    "    draw_critic_norm = list()\n",
    "    draw_actor_norm = list()\n",
    "    \n",
    "    #storing terminate statistics\n",
    "    terminate_statistics = list()\n",
    "    TERMINATE_TEST_SIZE = 10\n",
    "    CHECK_FOR_PROFIT_TIMES = 500\n",
    "    \n",
    "    time_write = open(\"time_check\", \"w\")\n",
    "    \n",
    "    for i in tqdm(range(MAX_EPISODES), file=time_write):\n",
    "        \n",
    "        log_file.flush()\n",
    "              \n",
    "        if(i == 1):\n",
    "            checker = TerminateChecker(eps_reward[0], 0.05, TERMINATE_TEST_SIZE, CHECK_FOR_PROFIT_TIMES)\n",
    "        \n",
    "        #CHECK FOR TERMINATION(MA|EMA)\n",
    "        if(i >= TERMINATE_TEST_SIZE):\n",
    "            if(checker.terminate_check()):\n",
    "                print(\"#\" * 50)\n",
    "                print(\"STOP TRAINING in {} episodes\".format(str(i + 1)))\n",
    "                print(\"Weights are:\", grad_weights)\n",
    "                break\n",
    "            \n",
    "            terminate_statistics.append(np.array([checker.ma, checker.ema]))\n",
    "            fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 15))\n",
    "            ax[0].set_title(\"#Episode/Reward\")\n",
    "            ax[0].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[0].set_ylabel(\"Reward\", fontsize=14)    \n",
    "            ax[0].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), eps_reward[-len(terminate_statistics):])\n",
    "            \n",
    "            ax[1].set_title(\"#Episode/MA value\")\n",
    "            ax[1].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[1].set_ylabel(\"MA Value\", fontsize=14)    \n",
    "            ax[1].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), np.array(terminate_statistics)[:, 0])\n",
    "            start_x = TERMINATE_TEST_SIZE\n",
    "            finish_x = len(terminate_statistics) + TERMINATE_TEST_SIZE\n",
    "            ax[1].plot([start_x, finish_x], [checker.best_ma, checker.best_ma], color=\"red\")\n",
    "            ax[1].vlines(np.argmax(np.array(terminate_statistics)[:, 0])+(TERMINATE_TEST_SIZE), np.min(eps_reward), np.max(eps_reward), color=\"green\")    \n",
    "            ax[1].set_ylim([np.min(eps_reward)-100, np.max(eps_reward)+100])\n",
    "            \n",
    "            ax[2].set_title(\"#Episode/EMA value\")\n",
    "            ax[2].set_xlabel(\"Episode\", fontsize=14)\n",
    "            ax[2].set_ylabel(\"EMA Value\", fontsize=14)    \n",
    "            ax[2].plot(np.arange(len(terminate_statistics)) + (TERMINATE_TEST_SIZE), np.array(terminate_statistics)[:, 1])\n",
    "            ax[2].plot([start_x, finish_x], [checker.best_ema, checker.best_ema], color=\"red\")\n",
    "            ax[2].set_ylim([np.min(eps_reward)-100, np.max(eps_reward)+100])\n",
    "            ax[2].vlines(np.argmax(np.array(terminate_statistics)[:, 1])+(TERMINATE_TEST_SIZE), np.min(eps_reward), np.max(eps_reward), color=\"green\")\n",
    "                \n",
    "            plt.savefig(save_plots_dir + \"/term_stats/terminate_statistics_plot_\" + str(grad_weights) + \".png\")\n",
    "        \n",
    "        noise_level *= 0.5\n",
    "        noise_level = max(1e-7,noise_level - 1e-4)\n",
    "        \n",
    "        #for plotting\n",
    "        grad_global_norm = list()\n",
    "        grad_layers_norm = list()\n",
    "        grad_critic_norm = list()\n",
    "        grad_actor_norm = list()\n",
    "        REWARD_PLOT_TIMESTEP = 1\n",
    "        ACTION_PLOT_TIMESTEP = 1\n",
    "        #constant for plotting smoothed reward(averaged by last SMOOTH_REWARD episodes)\n",
    "        SMOOTH_REWARD = 5\n",
    "        predicted_actions = list()\n",
    "        noise_actions = list()\n",
    "\n",
    "        ep_reward = np.zeros(REWARD_SPACE_DIM)\n",
    "        ep_ave_max_q = np.zeros(REWARD_SPACE_DIM)\n",
    "        \n",
    "        state_reward = list()\n",
    "        loss = list()\n",
    "        \n",
    "        s = env.reset()\n",
    "        for j in range(MAX_EP_STEPS):\n",
    "\n",
    "            if RENDER_ENV:\n",
    "                env.render()\n",
    "\n",
    "            # Add an exploration noise\n",
    "            predicted_action = actor.predict(np.reshape(s, (1, State_size)))\n",
    "            noise_action = actor.noise.one(action_dim, noise_level)\n",
    "            a = np.clip(predicted_action + noise_action, -action_bound, action_bound)\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "            r *= REWARD_BURN_CONST\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n",
    "                              terminal, np.reshape(s2, (actor.s_dim,)))\n",
    "            \n",
    "            if(i % ACTION_PLOT_TIMESTEP == 0):\n",
    "                predicted_actions.append(predicted_action[0])\n",
    "                noise_actions.append(noise_action)\n",
    "                state_reward.append(r * (GAMMA ** j) / REWARD_BURN_CONST)\n",
    "            \n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            calculated_grad = False\n",
    "            \n",
    "            if replay_buffer.size() >= MINIBATCH_SIZE:\n",
    "                \n",
    "                for sample_num in range(R):\n",
    "                    \n",
    "                    s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(batch_size=MINIBATCH_SIZE)\n",
    "                    # Calculate targets\n",
    "                    target_q = critic.predict_target(\n",
    "                        s2_batch, actor.predict_target(s2_batch))\n",
    "\n",
    "                    y_i = []\n",
    "                    for k in range(MINIBATCH_SIZE):\n",
    "                        if t_batch[k]:\n",
    "                            y_i.append(r_batch[k])\n",
    "                        else:\n",
    "                            y_i.append(r_batch[k] + GAMMA * target_q[k])\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                    predicted_q_value, _ = critic.train(\n",
    "                        s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, REWARD_SPACE_DIM)))\n",
    "                    \n",
    "                    gain_predictions = np.mean(critic.predict(s_batch, actor.predict(s_batch)), axis=0)\n",
    "                    \n",
    "                    loss.append(np.mean((gain_predictions).dot(grad_weights)))\n",
    "                    \n",
    "                # Update the actor policy using the sampled gradient\n",
    "                    a_outs = actor.predict(s_batch)\n",
    "                    grads, grad_norm = critic.action_gradients(s_batch, a_outs, np.reshape(grad_weights, (REWARD_SPACE_DIM, 1)))\n",
    "                    grad_critic_norm.append(grad_norm)\n",
    "                    \n",
    "                    #train actor only one time \n",
    "                    if(sample_num == R - 1):\n",
    "                        _, global_norm, mu_norm = actor.train(s_batch, grads[0])\n",
    "                        grad_global_norm.append(global_norm)\n",
    "                        grad_actor_norm.append(mu_norm)\n",
    "                    \n",
    "                    if(sample_num == R - 1):\n",
    "                        actor.update_target_network()\n",
    "                        \n",
    "                    critic.update_target_network()\n",
    "            \n",
    "            s = s2\n",
    "            ep_reward += (np.array(r) * (GAMMA ** j)) / REWARD_BURN_CONST\n",
    "                \n",
    "            if(terminal):\n",
    "                cost = np.sum(ep_reward * grad_weights)\n",
    "                reward_timestep_sum += cost\n",
    "                \n",
    "                #for debug\n",
    "                if(i % 5 == 0):\n",
    "                    log_file.write(\"EPISODE \" + str(i) +\"\\n\" +\" REWARD: \"+str(ep_reward)+\"\\n WEIGHTED REWARD:\" + str(cost) + \"\\n\")\n",
    "                \n",
    "                if(cost > best_cost):\n",
    "                    best_cost = cost\n",
    "                    best_rewards = ep_reward\n",
    "                \n",
    "                ###DRAW PLOTS BEGINING\n",
    "                if(i % REWARD_PLOT_TIMESTEP == 0):\n",
    "                    eps_reward.append(reward_timestep_sum / REWARD_PLOT_TIMESTEP)\n",
    "                    fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 12))\n",
    "                    ax[0].set_title(\"Episode/Cumulative Reward\", fontsize=18)\n",
    "                    ax[0].set_xlabel(\"Episode\", fontsize=14)\n",
    "                    ax[0].set_ylabel(\"Cumulative Reward\", fontsize=14)\n",
    "                    ax[0].plot((np.arange(len(eps_reward)) + 1) * REWARD_PLOT_TIMESTEP, eps_reward)\n",
    "                    reward_timestep_sum = 0\n",
    "                    \n",
    "                    #eps_reward.append(reward_timestep_sum / REWARD_PLOT_TIMESTEP)\n",
    "                    ax[1].set_title(\"Episode/Cumulative Reward\", fontsize=18)\n",
    "                    ax[1].set_xlabel(\"Episode\", fontsize=14)\n",
    "                    ax[1].set_ylabel(\"LOG(Cumulative Reward)\", fontsize=14)\n",
    "                    ax[1].plot((np.arange(len(eps_reward)) + 1) * REWARD_PLOT_TIMESTEP, -np.log10(-np.array(eps_reward)))\n",
    "                    reward_timestep_sum = 0\n",
    "                    plt.savefig(save_plots_dir + \"/reward_plots/episodes_reward_plot\")\n",
    "                \n",
    "                if((i + 1) % SMOOTH_REWARD == 0 and (i + 1) >= SMOOTH_REWARD):\n",
    "                    smoothed_reward = np.mean(np.array(eps_reward).reshape((len(eps_reward) // SMOOTH_REWARD, SMOOTH_REWARD)), axis=1)\n",
    "                    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "                    ax.set_title(\"Episode/Smoothed Cumulative Reward\", fontsize=18)\n",
    "                    ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                    ax.set_ylabel(\"Smoothed Cumulative Reward\", fontsize=14)\n",
    "                    ax.plot(np.arange(smoothed_reward.shape[0]) * SMOOTH_REWARD + 1, smoothed_reward)\n",
    "                    plt.savefig(save_plots_dir + \"/reward_plots/smoothed_episodes_reward_plot\")\n",
    "                \n",
    "                if(i % ACTION_PLOT_TIMESTEP == 0):\n",
    "                    figure, ax = plt.subplots(nrows=3, ncols=1,figsize=(20, 15))\n",
    "                    colors = ['red', 'blue', 'green', 'orange']                \n",
    "                    ax[0].set_title(\"Predicted actions\", fontsize=16)\n",
    "                    ax[0].set_ylabel(\"Action value\", fontsize=14)\n",
    "                    \n",
    "                    for d in range(action_dim):\n",
    "                        ax[0].plot(np.array(predicted_actions)[:, d], label=\"predicted_action_\" +str(d + 1), color=colors[d])\n",
    "                    ax[0].legend(loc='best')\n",
    "                    \n",
    "                    ax[1].set_title(\"Noise actions\", fontsize=16)\n",
    "                    ax[1].set_ylabel(\"Action value\", fontsize=14)\n",
    "                    \n",
    "                    for d in range(action_dim):\n",
    "                        ax[1].plot(np.array(noise_actions)[:, d], label=\"noise_action_\" + str(d + 1), color=colors[d])     \n",
    "                    ax[1].legend(loc='best')\n",
    "                    \n",
    "                    \n",
    "                    colors = [\"red\", \"blue\"]\n",
    "                    for d in range(action_dim):       \n",
    "                        ax[2].plot(np.array(state_reward)[:, d], label=\"step_reward_{}_objective\".format(d + 1), color=colors[d])\n",
    "                    ax[2].set_title(\"Episode Step/Reward\", fontsize=16)\n",
    "                    ax[2].set_xlabel(\"Episode step\", fontsize=14)\n",
    "                    ax[2].set_ylabel(\"Immediate Reward\", fontsize=14)\n",
    "                    ax[2].legend(loc='best')                       \n",
    "                    plt.savefig(save_plots_dir + \"/action_plots/actions_visualization_\" + str(i) + \"episode\")\n",
    "                \n",
    "                fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(16, 20))\n",
    "                \n",
    "                draw_global_norm.append(np.mean(np.array(grad_global_norm)))\n",
    "                ax[0].plot(draw_global_norm)\n",
    "                ax[0].set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax[0].set_ylabel(\"Average gradient norm\", fontsize=14)\n",
    "                ax[0].set_title(\"#Episode/Full Gradient Norm\", fontsize=14)\n",
    "                \n",
    "                if(i % 5 == 0):\n",
    "                    log_file.write(\"EPISODE \" + str(i) +\"\\n Average Gradient Norm: \" + str(draw_global_norm[-1]) + \"\\n\")\n",
    "                \n",
    "                draw_critic_norm.append(np.mean(np.array(grad_critic_norm)))\n",
    "                ax[1].plot(draw_critic_norm)\n",
    "                ax[1].set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax[1].set_ylabel(\"Average gradient norm\", fontsize=14)\n",
    "                ax[1].set_title(\"#Episode/ dQ/da norm\", fontsize=14)\n",
    "                \n",
    "                draw_actor_norm.append(np.mean(np.array(grad_actor_norm)))\n",
    "                ax[2].plot(draw_actor_norm)\n",
    "                ax[2].set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax[2].set_ylabel(\"Average gradient norm\", fontsize=14)\n",
    "                ax[2].set_title(\"#Episode/ d(mu)/d(theta) norm\", fontsize=14)\n",
    "                \n",
    "                plt.savefig(save_plots_dir + \"/gradient_norm_plots/episodes_gradient_norm_plot\")\n",
    "                \n",
    "                draw_loss.append(-np.mean(np.array(loss)))\n",
    "                fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 6))\n",
    "                ax.set_title(\"#Episode/Loss\")\n",
    "                ax.set_xlabel(\"Episode\", fontsize=14)\n",
    "                ax.set_ylabel(\"Loss\", fontsize=14)\n",
    "                ax.plot(draw_loss)\n",
    "                plt.savefig(save_plots_dir + \"/loss_plots/episodes_loss_plot\")\n",
    "                \n",
    "                plt.close(\"all\")\n",
    "                ### DRAW PLOTS ENDING\n",
    "                            \n",
    "                #Update Statistics\n",
    "                if(i >= 1):\n",
    "                    checker.change_ma(eps_reward[-1])\n",
    "                    checker.change_ema(eps_reward[-1])       \n",
    "                break\n",
    "                \n",
    "    return best_rewards\n",
    "                \n",
    "def test(weights, actor, test_episodes_num, action_bound):\n",
    "    \n",
    "    video_cal = lambda x : x % 5 == 0\n",
    "\n",
    "    env = LQR(10)\n",
    "    \n",
    "    eps_reward = list()\n",
    "    \n",
    "    print(\"!!!!!TEST WITH WEIGHTS : \", weights)\n",
    "    \n",
    "    for i in range(test_episodes_num):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        cum_reward = np.zeros(REWARD_SPACE_DIM)\n",
    "        \n",
    "        for j in range(MAX_EP_STEPS):\n",
    "            \n",
    "            if (RENDER_ENV):\n",
    "                env.render()\n",
    "        \n",
    "            a = actor.predict(np.reshape(s, (1, State_size)))\n",
    "            a = np.clip(a, -action_bound, action_bound)\n",
    "            s2, r, terminal, info = env.step(a[0])\n",
    "            \n",
    "            cum_reward += (r * (GAMMA ** j))\n",
    "            \n",
    "            s = s2\n",
    "            \n",
    "            if(terminal):\n",
    "                eps_reward.append(cum_reward)\n",
    "                break\n",
    "                \n",
    "    eps_reward = np.mean(eps_reward, axis=0)\n",
    "    \n",
    "    return eps_reward\n",
    "        \n",
    "def main(file_name):\n",
    "    \n",
    "    best_rewards = dict()\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #debug info (weighted cummulative episode reward)\n",
    "        log_file = open(file_name, \"w\")\n",
    "\n",
    "        env = LQR(10)\n",
    "    \n",
    "        state_dim = 2\n",
    "        action_dim = 2\n",
    "        action_bound = env.action_space.high\n",
    "        \n",
    "        for ind, w in tqdm(enumerate(gradient_weights)):\n",
    "            np.random.seed(RANDOM_SEED)\n",
    "            tf.set_random_seed(RANDOM_SEED)\n",
    "            env.seed(RANDOM_SEED)\n",
    "            env = LQR(10)\n",
    "            actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n",
    "                             ACTOR_LEARNING_RATE, TAU)\n",
    "\n",
    "            critic = CriticNetwork(sess, state_dim, action_dim,\n",
    "                               CRITIC_LEARNING_RATE, TAU, actor.get_num_trainable_vars())\n",
    "        \n",
    "            actor.noise = f2_noise.one_fsq_noise()\n",
    "        \n",
    "            video_cal = lambda x : x % 20 == 0\n",
    "        \n",
    "            if GYM_MONITOR_EN:\n",
    "                if not RENDER_ENV:\n",
    "                    env = wrappers.Monitor(\n",
    "                        env, MONITOR_DIR, video_callable=False, force=True)\n",
    "                else:\n",
    "                    env = wrappers.Monitor(env, MONITOR_DIR, video_callable=video_cal, force=True)\n",
    "            \n",
    "            best_rewards[str(w)] = train(sess, env, actor, critic, file_name, action_dim, action_bound, w, log_file)\n",
    "           \n",
    "            env.close()\n",
    "            pareto[str(w)] = test(w, actor, MAX_EPISODES_TEST, action_bound)\n",
    "            front = remove_dominated(np.array([y for y in pareto.values()]))\n",
    "            np.save(save_plots_dir + \"/pareto_front_progress/\" + str(ind + 1) + str(\"_weights_samples\"), front)\n",
    "            \n",
    "            file_pareto = open(save_plots_dir + \"/pareto_dict\", \"w\")\n",
    "            file_pareto.write(str(pareto))\n",
    "            file_pareto.close()\n",
    "            \n",
    "            best_front = remove_dominated(np.array([y for y in best_rewards.values()]))\n",
    "            file_best_rewards = open(save_plots_dir + \"/best_rewards_dict\", \"w\")\n",
    "            file_best_rewards.write(str(best_rewards))\n",
    "            file_best_rewards.close()\n",
    "            np.save(save_plots_dir + \"/pareto_front_progress/\" + str(ind + 1) + str(\"_weights_samples_best\"), best_front)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Max Number of Training Episodes\n",
    "MAX_EPISODES = 2500 \n",
    "#Number ot test episodes\n",
    "MAX_EPISODES_TEST = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing all found policies\n",
    "pareto = dict()\n",
    "\n",
    "main(save_plots_dir + \"/log.txt\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
